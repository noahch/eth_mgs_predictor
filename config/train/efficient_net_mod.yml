config:
  name: efficient_net_mod # name of the configuration file
basic:
  # The Cuda device ID, that is needed to enable GPU acceleration. Can also be several IDs seperated by Comma
  cuda_device_name: cuda:0
  # The name of the experiment
  experiment_name: EfficientNet
  # The description and purpose of the experiment
  experiment_description: Train MGS on EfficientNet
  # The path to the result directory
  result_directory: "C:\\Users\\Hackerman\\Documents\\Noah\\results"
  # The name of the result folder
  result_directory_name: ''
  # if wandB should be enabled (0 = not enabled, 1 = enabled)
  enable_wand_reporting: 1
model:
  # The name of the model
  name: efficient_net_mod
  # If pretrained weights should be used (0 = false, 1 = true)
  pretrained: 1
  # name of the underlying timm model & weights (e.g. tf_efficientnet_b0, efficientnetv2_rw_t)
  timm_model_name: 'efficientnetv2_rw_t'
  # The rate of dropout (0.2-0.5 is recommended, only matters if chosen model has a dropout layer)
  dropout: 0
  # chose whether model is trained on MGS Attributes or on Pain-Only
  mgs_attributes: 1
  # whether mgs attributes should be one hot encoded
  mgs_one_hot: 1
  # freeze all layers except last
  freeze_layers: 1
dataset:
  # Path to the partition file
  partition_filename: "C:\\Users\\Hackerman\\Documents\\Noah\\BMv1\\partitions.csv"
  # Path to the labels' file
  dataset_labels_filename: "C:\\Users\\Hackerman\\Documents\\Noah\\BMv1\\labels_rand_mod.csv"
  # Path to the folder which contains the images
  dataset_image_folder: "C:\\Users\\Hackerman\\Documents\\Noah\\BMv1\\images"
training:
  # How many epochs the model should be trained on
  epochs: 35
  # How often the to save the model's weight during training (e.g. 10 safes the model's weights every 10 epochs)
  save_frequency: 5
  optimizer:
    # Type of Optimizer (e.g. SGD for stochastic gradient descent)
    type: Adam
    # Learning rate (e.g. 0.001, 0.01, 0.1, 1)
    learning_rate: 0.0001
    # Momentum
    momentum: 0
    # beta1
    beta1: 0.9
    # beta1
    beta2: 0.999
    # epsilon
    epsilon: 1e-07
    # weight decay
    weight_decay: 0
  criterion:
    # Loss function that should be used (e.g. "BCEWithLogitsLoss")
    type: CrossEntropyLoss
  lr_scheduler:
    # Type of learning rate scheduler that adjusts the Learning rate dynamically during training (e.g. "ReduceLROnPlateau")
    type: ReduceLROnPlateau
    # LRscheduler: after how many epochs the learning rate is adjusted
    step_size: 4
    # multiplicator of learning rate. (e.g. new learning rate = old learning rate * gamma)
    gamma: 0.1
    # How many epochs to wait while the validation loss does not decrease before adjusting the learning rate
    patience: 2
  dataloader:
    # Batch size of training and validation data (training data is split in equal sets of size batch size)
    batch_size: 32
    # If the data is shuffled before it is split in batches (True for shuffling and False for not shuffling)
    shuffle: 'True'
    # how many images to preprocess at the same time (>1 uses multiprocessing, suggested around 8 if training on 1 gpu)
    num_workers: 8
    # How many batches are preprocessed on each worker
    prefetch_factor: 20
  save_preprocessed_image:
    # If enabled, saves images in defined frequency
    enabled: 0
    frequency: 1000

